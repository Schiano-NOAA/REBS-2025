<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>a_assessment-configuration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="04a_assessment-configuration_files/libs/clipboard/clipboard.min.js"></script>
<script src="04a_assessment-configuration_files/libs/quarto-html/quarto.js"></script>
<script src="04a_assessment-configuration_files/libs/quarto-html/popper.min.js"></script>
<script src="04a_assessment-configuration_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="04a_assessment-configuration_files/libs/quarto-html/anchor.min.js"></script>
<link href="04a_assessment-configuration_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04a_assessment-configuration_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="04a_assessment-configuration_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="04a_assessment-configuration_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="04a_assessment-configuration_files/libs/bootstrap/bootstrap-1bc8a17f135ab3d594c857e9f48e611b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="assessment-model" class="level1">
<h1>Assessment Model</h1>
<section id="history-of-modeling-approaches" class="level2">
<h2 class="anchored" data-anchor-id="history-of-modeling-approaches">History of Modeling Approaches</h2>
<p>Rougheye Rockfish (not including Blackspotted) on the U.S. Pacific Coast was first evaluated in 2010 by <span class="citation" data-cites="dick_estimates_2010">@dick_estimates_2010</span> using depletion-based stock reduction analysis (DB-SRA), as Category 3 stock. That model estimated the population had greater than a 50% probability of exceeding the estimated proxy overfishing level in 2010 if the harvest remained at the observed levels. DB-SRA estimated a proxy OFL for Rougheye Rockfish of 78.7 mt with a 95% confidence interval between 4.7-587 metric tons.</p>
<p>The first benchmark assessment for<code>r Spp</code> was conducted in 2013 <span class="citation" data-cites="hicks_status_2013">[@hicks_status_2013]</span>. A 2013 benchmark stock assessment used Stock Synthesis (version 3.24O) integrated statistical catch-at-age model, which is different from the delay-difference model with an assumed stock status prior DB-SRA analysis used in 2010. The stock assessment has been used for management as a Category 2 stock assessment. The 2013 assessment used a substantially updated catch history, indices of abundance, and biological compositions (lengths and ages). The natural mortality value was also updated to be higher value than the one used in the DB-SRA model. The 2013 assessment also assumed logistic selectivity for all fleets and surveys, except for Triennial Shelf Survey, which was allowed to be dome-shaped. With higher natural mortality and asymptotic selectivity assumptions, the 2013 assessment estimated 2013 spawning biomass to be at 47% relative to unfished equilibrium spawning biomass, with a 95% confidence interval between 30.5% - 64.2%. The 2013 spawning biomass was estimated to be 2,552 metric tons, with a 95% confidence interval between 1,024 - 4,081 metric tons.</p>
<p>With this new benchmark assessment, we re-evaluate all the data sources available for <code>r Spp</code>, analyse new and re-analyse previously used data with current statistical methods and best practices, and re-evaluate modelling assumptions. Detailed description of changes made since 2013 benchamrk assessment is provided in <a href="#sec-bridging" class="quarto-xref">Section&nbsp;1.3</a>.</p>
</section>
<section id="response-to-most-recent-star-panel-recommendations" class="level2">
<h2 class="anchored" data-anchor-id="response-to-most-recent-star-panel-recommendations">Response to Most Recent STAR Panel Recommendations</h2>
<p>There were several recommendations from the 2013 STAR panel, broken into two categories</p>
<section id="general-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="general-recommendations">General recommendations</h3>
<ol type="1">
<li><em>Investigate data-weighting options.</em> This has been an ongoing research topic in stock assessments over the last several decades <span class="citation" data-cites="francis_data_2011 thorson_model-based_2017 puntInsightsDataWeighting2017a">[@francis_data_2011; @thorson_model-based_2017; @puntInsightsDataWeighting2017a]</span>. In this assessment, we use <span class="citation" data-cites="francis_data_2011">@francis_data_2011</span> method, and explore other methods of data weighting within sensitivity analysis described in <strong>?@sec-assmt-sens</strong>.</li>
<li><em>A workshop for constructing abundance indices from survey GLMMs.</em> This is another topic that has developed greatly since this time <span class="citation" data-cites="thorson_geostatistical_2015">[@thorson_geostatistical_2015]</span>. Spatio-temporal models used in this assessment are described in <strong>?@sec-surveys</strong>.</li>
<li><em>Continue collection of ages.</em> This had been done, and this assessment benefits from several more years of age data.</li>
<li><em>Exploring historical catches.</em> This again has been an ongoing topic and addressed for many of our groundfishes. We use the latest estimates in this assessment.</li>
<li><em>SSC guidance on decision tables.</em> Decision table discussion evolve after every stock assessment cycle, and we are using the latest approaches to decision tables in this assessment.</li>
<li><em>Investigate fishery-independent slope surveys, such as submersibles.</em> These surveys are still not available for slope species.</li>
</ol>
</section>
<section id="stock-specific-recommendations" class="level3">
<h3 class="anchored" data-anchor-id="stock-specific-recommendations">Stock-specific recommendations</h3>
<ol type="1">
<li><em>Collecting additional age data.</em> This has been done and included in this stock assessment.</li>
<li><em>Collecting genetic material to explore distinguishing Rougheye and Blackspotted Rockfishes.</em> This work has been done as was presented earlier in the document when discussing stock structure decisions.</li>
<li><em>The cause of the re-occurring decrease in sizes around 40cm.</em> The data continue to exhibit a lack of 35-40cm fish in fleets that catch smaller fish (WCGBTS and Triennial Survey, and bottom trawl discard fleet), creating bimodal distributions of length data (<strong>?@fig-agg-len-fit</strong>). We found that this pattern is not caused by the relative depth distribution of two stocks within the complex, with Rougheye Rockfish being shallower and Blackspotted Rockfish deeper. <strong>?@fig-bimodal-lengths</strong> (pers. comm. P. Frey, NWFSC) shows the length distribution of Rougheye Rockfish and Blackspotted Rockfish identified to species using genetics, and the Rougheye Rockfish still shows a bimodal length distribution. Further analysis reveals that this bimodal length distribution is not limited to <code>r Spp</code>, but is also evident in multiple other rockfish species. We continue to explore data to understand mechanisms behind this pattern, which are potentially related to accessibility of trawl gear to habitats specific to that size group. In this assessment we were able to fit the bimodal length distribution by allowing surveys’ selectivity curves more flexibility, and not fixing it asymptotic as in the previous assessment.</li>
<li><em>Additional maturity and fecundity studies.</em> While no fecundity studies are available, updated maturity is presented in the maturity section of the document.</li>
<li><em>Age validation.</em> While no age validation study has been completed, the age readers are confident what annuli represent a year’s worth of growth. Multiple ages are available and ageing error is characterized in this stock assessment.</li>
<li><em>Understanding stock structure.</em> Discussed in the <strong>?@sec-stock_structure</strong> of this document.</li>
<li><em>Connectivity of stocks across the species ranges.</em> This is also discussed in the <strong>?@sec-stock_structure</strong> of the document.</li>
</ol>
</section>
</section>
<section id="sec-bridging" class="level2">
<h2 class="anchored" data-anchor-id="sec-bridging">Model Changes from the Last Assessment and Bridging Analysis</h2>
<p>The last full assessment of <code>r Spp</code> was conducted in 2013. The 2013 assessment model was the starting point for this assessment. We included a number of improvements related to use of data, model structure and modeling techniques. Below, we describe the most important changes made since the last assessment:</p>
<ul>
<li>Upgraded the model to Stock Synthesis 3.30.22.1 version. This is standard practice to capitalize on newly developed features and corrections to older versions as well as improvements in computational efficiency. The list of changes made to Stock Synthesis since 2013 can be found in the model <a href="https://github.com/orgs/nmfs-ost/projects/11">change log</a>. No discernible differences were produce by this change. The status (<strong>?@fig-RSS_2013</strong>) and scale (<strong>?@fig-SO_2013</strong>) of both models are exactly the same, as are the estimates of within model uncertainty.</li>
<li>Specified a two-sex model, instead of one-sex model, to allow sex-specific estimation of natural mortality and growth. No discernible differences were produce by this change either (<strong>?@fig-Sex1vs2_SO</strong> and <strong>?@fig-Sex1vs2_Bratio</strong>).</li>
<li>Included bottom trawl and non-trawl discards as separate fleets (see <a href="#sec-fleet" class="quarto-xref">Section&nbsp;1.4.3</a> for details), instead of treating them as part of the same fleets as landings. Results did not impact the model output (<strong>?@fig-Discard_comp_SO</strong> and <strong>?@fig-Discard_comp_RSS</strong>).</li>
<li>Split mid-water trawl catches from bottom trawl landings and treat them as a separate fleet (see <a href="#sec-fleet" class="quarto-xref">Section&nbsp;1.4.3</a> for details), to account for gradually increasing contribution of mid-water trawl catches. Results did not impact the model output.</li>
<li>Updated historical and current fishery removals, to include most up to date information. Since 2013 assessment, WDFW completed historical catch reconstruction of rockfish, and newly estimated landings represent improvement. For the period between 1987 and 1999, Oregon PacFIN landings were supplemented with the additional estimates of <code>r Spp</code>landings reported within unspecified rockfish market categories. Results did not impact the model output.</li>
<li>Recalculated survey abundance indices using sdmTMB geostatistical model. Results did not impact the model output.</li>
<li>Added more biological compositions, mainly in years since 2013, but also some historical ages. Adding more composition data resulted in sight increase in stock scale (<strong>?@fig-SO_Bridging</strong>).</li>
<li>Updated input sample sizes associated with fisheries and survey length composition data to using a function of number of trips and number and fish (rather than number of trips and number of hauls, as in previous assessment), to follow current best practices and ensure a consistent treatment of fishery and survey input data.</li>
<li>Updated ageing error matrices.</li>
<li>Updated weight-length, maturity and fecundity parameters, to include most up to date and improved information.</li>
<li>Updated spawn-recruit parameters with Beverton-Holt steepness fixed at 0.72, and recruitment variability at 0.5 for consistency with the calculated recruitment variability in the model.</li>
<li>Assumed natural mortality for males consistent with maximum ages of observed for <code>r Spp</code>, while estimating female natural mortality using the <span class="citation" data-cites="hamel_development_2022">@hamel_development_2022</span> prior. Previously, natural mortality for both sexes was estimated within the model, but estimated values were higher than expected for maximum ages observed for this stock. This change reduced the natural mortality values for both sexes, which resulted in decreased scale of the stock.</li>
<li>Provided flexibility for the bottom trawl fleet and bottom trawl surveys to estimate dome-shaped selectivity (<strong>?@fig-sel-tv-comp</strong>), previously assumed asymptotic. This change was prompted by the lack of fit to length compositions data. Also, the examination of mean fish lengths by fleet indicated that the bottom trawl fleet capture smaller fish than mid-water trawl and non-trawl gear. This change resulted in a substantial increase of stock scale, and is considered an improvement to the model structure. We also allowed estimated selectivity to vary with time in both the bottom trawl and non-trawl fleets, to account for management changes that can impact selectivity of these fleets.</li>
</ul>
<p>Bridging analysis was conducted to illustrate the impacts of incremental changes. With the new fecundity parameters, the 2025 model produces spawning output (in millions of eggs) rather than spawning biomass (in mt as in 2013 model); so with different metrics, these outputs were no longer comparable between the two models. However, we ran the 2013 model with new fecundity parameters to allow for direct comparison of the results from the bridging runs. The bridging analysis, with the most influential steps done sequentially, is shown in <strong>?@fig-SO_Bridging</strong> and <strong>?@fig-Bratio_Bridging</strong>.</p>
<p>This assessment (compared to 2013 assessment) estimates much higher stock scale, primarily, as discussed above, due to changes in treatment of selectivity parameters and relaxing asymptotic selectivity assumptions for the bottom trawl fleet and bottom trawl surveys. Changes in selectivity assumptions allow to substantially improve fits to length and age composition data in fisheries and surveys. The stock scale was also affected by the changes in treatment of natural mortality, bringing the scale down to a lower level compared to the previously used natural mortality assumption.</p>
</section>
<section id="general-model-specifications" class="level2">
<h2 class="anchored" data-anchor-id="general-model-specifications">General Model Specifications</h2>
<section id="modelling-platform" class="level3">
<h3 class="anchored" data-anchor-id="modelling-platform">Modelling Platform</h3>
<p>Stock Synthesis statistical catch-at-age modelling framework <span class="citation" data-cites="MethotWetzel2013">[@MethotWetzel2013]</span>, version 3.30.23.1, is used for this assessment. This framework allows the integration of a variety of data types and model specifications. The Stock Assessment Continuum tool (https://github.com/shcaba/SS-DL-tool) was also used to explore model efficiency, likelihood profiling, retrospective analyses, and plotting sensitivities. The companion R package r4ss (version 1.51.0) along with R version 4.4.3 were used to investigate and plot model fits.</p>
</section>
<section id="model-structure" class="level3">
<h3 class="anchored" data-anchor-id="model-structure">Model Structure</h3>
<p>This stock assessment is for the <code>r Spp</code>, two species that form one management complex. Assessment area is from the U.S.-Mexican border om the south to the U.S. Canadian border on the north (<strong>?@fig-map</strong>). The assessment excludes consideration of the Puget Sound and Salish Sea. Within this area, the assessment treats the U.S. <code>r Spp</code> resource as a single coastwide stock.</p>
<p>This is a sex-specific model. The sex-ratio at birth is assumed to be 1:1. Females and males have separate growth curves (fully estimated within the model) and sex-specific weight-at-length parameters. The model assumes a constant natural mortality of 0.036 yr-1 for males, while natural mortality for females is estimated based on <span class="citation" data-cites="hamel_development_2022">@hamel_development_2022</span>. The length frequency distributions are represented as thirty six 2-cm bins ranging between 10 and 80 cm. Population length bins are defined at a finer 2-cm scale, ranging between 4 and 84 cm. Age data is included as conditional age-at length compositions with bins ranging between 1 and 100 years.</p>
<p>The modeling period begins in 1892, and the stock prior to that is assumed to be in an unfished equilibrium condition.</p>
</section>
<section id="sec-fleet" class="level3">
<h3 class="anchored" data-anchor-id="sec-fleet">Fleet Definitions</h3>
<p>The model is structured to track six fleets and include data from four surveys.</p>
<p>Defining fleets is largely based on differing fleet selectivity (i.e., how the fishery captures fish by length and/or age). In the stock assessment model, selectivity translates into how the removals are taken via length and/or age out of the population. In this assessment, the following fleet structure is being used to model commercial fishery removals:</p>
<ul>
<li>Fleet 1: Commercial bottom trawl fishery.</li>
<li>Fleet 2: Dead discard from bottom trawl fishery.</li>
<li>Fleet 3: Commercial non-trawl (mainly the long-line) fishery.</li>
<li>Fleet 4: Dead discard from non-trawl fishery.</li>
<li>Fleet 5: Contemporary mid-water trawl fishery.</li>
<li>Fleet 6: At-sea hake fishery bycatch.</li>
</ul>
<p>In 2013 assessment <span class="citation" data-cites="hicks_status_2013">[@hicks_status_2013]</span>, fisheries removals were split among three fleets - trawl, hook-and-line and at-sea hake fishery bycatch. For the first two fleets (trawl and hook-and-line), removals were divided between landings and discards, with selectivity and retention curves estimated within the model.</p>
<p>In this assessment, we treat discards in trawl and non-trawl fisheries as separate fleets from landings fleets.</p>
<p>Treating discards as separates fleets from landings provides several advantages, including:</p>
<ul>
<li>With separate discard fleets, we can easily track relative amounts of landings and discards within a fishery (they are not being combined into the total catch).</li>
<li>This approach provides more flexibility to explore different selectivity assumptions for both landed and discarded fish dome-shaped vs asymptotic, mirroring one to the other, etc.</li>
<li>This approach allows to avoid hard to diagnose issues that come from estimating retention curves (especially with limited amount of data).</li>
<li>The biological data for landings and discards are collected independently (port sampling vs on-board observers), and using different sampling approaches. Treating landings and discards as separate fleets in the model allows us to weight those data sources separately as well, to balance the representation of samples.</li>
</ul>
<p>The change in treating discards as separate fleets does not impact model results (<strong>?@fig-Discard_comp_SO</strong> and <strong>?@fig-Discard_comp_RSS</strong>), regardless of the selectivity form being assumed for the discard fleets. Historically, no discarding was observed for <code>r Spp</code> <span class="citation" data-cites="pikitch_evaluation_1988">[@pikitch_evaluation_1988]</span>, see <strong>?@sec-historical_discard</strong> for details. The 2013 assessment estimated zero historical discard for both trawl and fixed gear fleets, based on the available data. In this assessment, therefore, we assume no discard until early 2000, when the first <code>r Spp</code> was observed after the introduction of trip limits for rockfish.</p>
<p>We also split trawl fishery data into bottom trawl and mid-water trawl fleets. Catch data indicates that contribution of mid-water trawl catches gradually grew over the past 20 years, and now they represent majority of the trawl removals (<strong>?@fig-landings</strong>). Historical information on mid-water catches of <code>r Spp</code> comes from <span class="citation" data-cites="pikitch_evaluation_1988">@pikitch_evaluation_1988</span>, which has no records of <code>r Spp</code> mid-water trawl catches, neither retained nor discarded. Also, Oregon historical catch reconstruction <span class="citation" data-cites="karnowski_historical_2014">[@karnowski_historical_2014]</span> has only one record of 0.0002 metric tons of <code>r Spp</code> taken in 1985, even though the mid-water trawl catches had their own market category in Oregon since the early-1980s <span class="citation" data-cites="karnowski_historical_2014">[@karnowski_historical_2014]</span>, and multiple rockfish species are reported as caught by this gear. This information suggest that historically <code>r Spp</code> mid-water catches were negligible.</p>
<p>As reported in <strong>?@sec-surveys</strong>, the following surveys are included in the model:</p>
<ul>
<li>Survey 1: West Coast Groundfish Bottom Trawl Survey (WCGBTS; 2003-2024)</li>
<li>Survey 2: Triennial (every three years) Survey (1980-2004)<br>
</li>
<li>Survey 3: Alaska Fishery Science Center (AFSC) Slope Survey (1997-2001)</li>
<li>Survey 4: Northwest Fisheries Science Center (NWFSC) Slope Survey (1999-2001)</li>
</ul>
<p>We use length-based selectivity curves for all fleets for this stock assessment model (as was done in the 2013 assessment), as there is no evidence that significant age-based selectivity is occurring. We considered logistic and dome-shaped selectivity options for various combinations of fleets and time periods during model development.</p>
</section>
<section id="model-likelihood-components" class="level3">
<h3 class="anchored" data-anchor-id="model-likelihood-components">Model Likelihood Components</h3>
<p>There are five primary likelihood components for each assessment model:</p>
<ol type="1">
<li>Fit to length composition samples.</li>
<li>Fit to age composition samples (all fit as conditional age-at-length).</li>
<li>Fit to survey indices of abundance.</li>
<li>Penalties on recruitment deviations (specified differently for each model).</li>
<li>Prior distribution penalties</li>
</ol>
<p>In addition, there is a catch component to the likelihood, but catches are essentially fit without error. Additionally, there is a crash penalty that is invoked if true catches would cause the stock to go extinct. The penalty would alter catches to avoid extinction, but any presence of a crash penalty is used as in indication that the model has been mispecified, so this likelihood contribution should always be 0.</p>
</section>
<section id="data-weighting" class="level3">
<h3 class="anchored" data-anchor-id="data-weighting">Data Weighting</h3>
<p>Initial sample sizes for the length and conditional age-at-length compositions were also considered for additional data-weighting. The method of <span class="citation" data-cites="francis_data_2011">@francis_data_2011</span>, specifically equation TA1.8, was used to re-weight the length and conditional age-at-length composition data against other inputs and likelihood components. The Francis method treats mean length and age as indices, with effective sample size defining the variance around the mean. If the variability around the mean does not encompass model predictions, the data should be down-weighted until predictions fit within the intervals. This method accounts for correlation in the data (i.e., the multinomial distribution), but can be sensitive to years that are outliers, as the amount of down-weighting is applied to all years within a data source, and are not year-specific. Sensitivities were performed examining different data-weighting treatments: 1) the Dirichlet-Multinomial approach <span class="citation" data-cites="thorson_model-based_2017">[@thorson_model-based_2017]</span>, 2) the McAllister-Ianelli Harmonic Mean approach <span class="citation" data-cites="mcallister_bayesian_1997">[@mcallister_bayesian_1997]</span>, or 3) no additional data-weighting.</p>
<p>The ability to estimate additional variance for indices allows the model to balance model fit to that data while acknowledging that variances may be underestimated in the index standardization. Given the large inputted variances and the limited contrast in the index trends did not require the consideration of further variance estimation. Removal of the index data was explored to demonstrate the limited influence of this data in the model.</p>
</section>
</section>
<section id="model-parameters" class="level2">
<h2 class="anchored" data-anchor-id="model-parameters">Model Parameters</h2>
<section id="estimated-and-fixed-parameters" class="level3">
<h3 class="anchored" data-anchor-id="estimated-and-fixed-parameters">Estimated and Fixed Parameters</h3>
<p>The full list of estimated and fixed parameters are found in <strong>?@tbl-param</strong>.</p>
<p>All growth parameters were estimable and did not change across the large majority of explored model scenarios, so they were estimated in the reference model. Natural mortality (<span class="math inline">\(M\)</span>) was not estimable for both sexes. When attempted, both values were estimated at values that caused the scale to approach the higher end of reasonable values, and thus not a risk neutral option. In order to balance model fit and reality, a likelihood profile was conducted on natural mortality for males (females <span class="math inline">\(M\)</span> being estimated) in order to find the lowest supported (i.e., within 2 negative log likelhood units) by the data male <span class="math inline">\(M\)</span> value. The profile shows conflicting information in the data, where lengths support higher natural mortality values and ages support lower natural mortality (<strong>?@fig-M-profile-explore</strong>). It is expected that ages would be more informative to natural mortality, which encourages considering just the age component likelihood. Most of the age components are not well informed for natural mortality, though the at-sea-hake fishery sampled age data does seem to be informative. This fishery has a logisitic selectivity, thus obtaining large and old individuals. Using this component likelihood, the value of 0.036 for male <span class="math inline">\(M\)</span> is the lowest value supported. The reference model this fixes male <span class="math inline">\(M\)</span> to this value and estimates female <span class="math inline">\(M\)</span>. Length-at-maturity, fecundity-weight, and length-weight relationship were all fixed, as is the only treatment option in SS3.</p>
<p>For recruitment, steepness (<span class="math inline">\(h\)</span>) was not estimable and was fixed to the rockfish prior of 0.72. Recruitment variability was set at 0.5 and checked for consistency with the calculated recruitment variability in the model. Recruitment deviations were estimated as a deviation not constrained to sum to 0, and initially estimated for the full time series. If the full time series was not estimated, the scale and status of the stock increased to unrealistically high levels, so the full time series estimation was retained. Given the longevity of these species, information on recruitment going deeper into the time series is not unreasonable.</p>
</section>
<section id="selectivity-assumptions" class="level3">
<h3 class="anchored" data-anchor-id="selectivity-assumptions">Selectivity Assumptions</h3>
<p>The selectivity of all fisheries and surveys were estimated either as logistic or dome-shaped selectivity. Blocks were also added as described in the data section. In the attempt to fit the biological data, it was found that bottom trawl fisheries, just as the trawl surveys were treated, only fit the data if the selectivity was domed. All fisheries that had final dome-shaped selectivity were given the flexibility to be logistic if it led to a better fit. The midwater, at-sea-hake and the final block of the non-trawl fisheries all required logistic selectivity to fit the data. The use of dome-shaped selectivity for the bottom trawl was a major difference from the previous stock assessment. The choice of selectivity for the bottom trawl survey changed the scale and status of the stock and therefore a major source of sensitivity.</p>
</section>
</section>
<section id="model-selection-and-key-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="model-selection-and-key-assumptions">Model Selection and Key Assumptions</h2>
<p>The reference model for <code>r spp</code> was developed to balance parsimony and realism, and the goal was to estimate a risk neutral spawning output trajectory and relative stock status for the stocks of <code>r spp</code> in state and federal waters off the U.S. West Coast. To achieve the above goals, the model uses different data types and sources to estimate reality, but relies on simplifying assumptions when the data are not informative to parameters. A series of investigative model runs were done to achieve the final reference model. Constructing integrated models (i.e., those fitting many data types) takes considerable model exploration using different configurations of the following treatments:</p>
<ul>
<li>Data types and weighting</li>
<li>Parameter treatments: which parameter can, cannot and do not need to be estimated</li>
<li>Phasing of parameter estimation</li>
<li>Exploration of local minima vs global minimum (see Model Convergence and Acceptability section below)</li>
</ul>
<p>Regarding data types, different biological data (i.e., length and/or age composition) with and without the catch time series (and no additional data weighting) were first included to obtain an understanding of the signal of stock status coming from the data (Figure XXXXXX). The length and age only models assume fixed life history values (growth fixed to external estimates, natural mortality assume the reference model values) and constant catch over the entire time series, while estimating the selectivity of each fleet. Under this constraint, the lengths suggest a stock status lower than the reference model, while the ages consider the stock is less depleted than supported by the ages (with no ageing error), and more similar to the reference model. Adding ageing error, Putting the two data sources together produce an intermediate stock status in the lower precautionary zone. Adding the catch time series substantially changes the stock status trajectory, with length or age only model above the reference stocks status. Combining the two came out just under the reference model. Only one model includes recruitment deviations, and demonstrates more dynamics behavior similar to that seen when biological compositions are unweighted (see Model Specification Sensitivities section ).</p>
<p>Stock scale was comparable once removal history was included, and demonstrates a large sensitivity to the scale of the stock given the data with no additional weighting included (Figure ).</p>
<p>Numerous exploratory models that included all data types and a variety of model specifications were subsequently explored and too numerous to fully report. In summary, the estimation of which life history parameters to estimate and fix was liberally explored.</p>
<p>The following is a list of things that were explored, typically in combination with one another</p>
<ul>
<li>Estimate or fix <span class="math inline">\(M\)</span></li>
<li>Estimate or fix any of the three growth parameter for each sex</li>
<li>Estimate or fix the stock-recruit relationship</li>
<li>Estimate or assume constant recruitment. If estimating recruitment, for what years?</li>
<li>Estimate additional survey variance, and for which survey?</li>
<li>Logistic or dome-shaped selectivity?</li>
<li>Estimate or fix selectivity parameters</li>
</ul>
<p>The biggest uncertainty was in the treatment of sex-specific <span class="math inline">\(M\)</span> and the selectivity of the bottom trawl fishery. The combination of these two sources covered the extent of all other sources of uncertainty observed and presented in the “Characterizing uncertainty” section of the document. The parameters uncertainty is different than the uncertainty derived from data treatment, such as ageing error and data-weighting. While these issues cause large uncertainty in stock scale and status estimates, the choice of treatments are based on the common challenge of balancing information content (i.e., what should the data be informing in the stock assessment) in the data within an integrated statistical frameworks. Those explorations are also provided in the “Characterizing uncertainty” section.</p>
<p>General attributes of the reference model are that indices of abundance are assumed to have lognormal measurement errors. Length compositions and conditional age at length samples are all assumed to follow a multinomial sampling distribution, where the sample size is fixed at the input sample size calculated during compositional example, and where this input sample size is subsequently reweighted to account for additional sources of overdispersion (see below). Recruitment deviations were also estimated are assumed to follow a lognormal distribution, where the standard deviation of this distribution is tuned as noted above.</p>
<p>Sensitivity scenarios and likelihood profiles (on <span class="math inline">\(lnR_0\)</span>, steepness, and natural mortality) were used to explore uncertainty in the above model specifications and are reported in the “Characterizing uncertainty” section.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>